{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading\n",
    "Multithreading is a programming concept where multiple threads (lightweight subprocesses) run concurrently within a single process. It enables a program to perform multiple tasks simultaneously, improving efficiency and responsiveness, especially for tasks that involve I/O operations or multiple cores of a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 2\n",
      "Number: 3\n",
      "Number: 4\n",
      "Number: 5\n",
      "Letter: a\n",
      "Letter: b\n",
      "Letter: c\n",
      "Letter: d\n",
      "Letter: e\n",
      "Execution time without threading: 10.02 seconds\n"
     ]
    }
   ],
   "source": [
    "## what is the multithreding\n",
    "## Wheen to use the multi threading\n",
    "### I/O bound task: Task that spend theat more time waiting for \n",
    "\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import time\n",
    "\n",
    "def print_number():\n",
    "    for i in range(1, 6):\n",
    "        print(f'Number: {i}')\n",
    "        time.sleep(1)  # Simulate a delay\n",
    "\n",
    "def print_letter():\n",
    "    for letter in 'abcde':\n",
    "        print(f'Letter: {letter}')\n",
    "        time.sleep(1)  # Simulate a delay\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "print_number()\n",
    "print_letter()\n",
    "finished_time = time.time()\n",
    "\n",
    "print(f\"Execution time without threading: {finished_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1\n",
      "Letter: a\n",
      "Number: 2\n",
      "Letter: b\n",
      "Number: 3\n",
      "Letter: c\n",
      "Letter: d\n",
      "Number: 4\n",
      "Letter: e\n",
      "Number: 5\n",
      "Execution time with threading: 5.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def print_number():\n",
    "    for i in range(1, 6):\n",
    "        print(f'Number: {i}')\n",
    "        time.sleep(1)  # Simulate a delay\n",
    "\n",
    "def print_letter():\n",
    "    for letter in 'abcde':\n",
    "        print(f'Letter: {letter}')\n",
    "        time.sleep(1)  # Simulate a delay\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create threads\n",
    "thread1 = threading.Thread(target=print_number)\n",
    "thread2 = threading.Thread(target=print_letter)\n",
    "\n",
    "# Start threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# Wait for threads to complete\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "finished_time = time.time()\n",
    "\n",
    "print(f\"Execution time with threading: {finished_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.07 seconds\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "def square_number():\n",
    "    for i in range(5):\n",
    "        time.sleep(1)  # Simulate a delay\n",
    "        print(f'Square: {i * i}')\n",
    "\n",
    "def cube_number():\n",
    "    for i in range(5):\n",
    "        time.sleep(1.5)  # Simulate a longer delay\n",
    "        print(f'Cube: {i ** 3}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create two processes\n",
    "    p1 = Process(target=square_number)\n",
    "    p2 = Process(target=cube_number)\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Start the processes\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    # Wait for processes to complete\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    finished_time = time.time() - start_time\n",
    "    print(f\"Execution time: {finished_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading with ThreadPoolExecutor\n",
    "Multithreading with ThreadPoolExecutor is a high-level way to manage and execute threads in Python using the concurrent.futures module. It simplifies thread management by allowing you to create a pool of threads to which you can submit tasks. This is particularly useful for I/O-bound tasks that involve operations like file reading, web scraping, or network requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 is starting...\n",
      "Task 1 is starting...\n",
      "Task 2 is starting...\n",
      "Task 0 is completed.\n",
      "Task 3 is starting...\n",
      "Result from Task 0\n",
      "Task 1 is completed.\n",
      "Task 4 is starting...\n",
      "Result from Task 1\n",
      "Task 2 is completed.\n",
      "Result from Task 2\n",
      "Task 3 is completed.\n",
      "Result from Task 3\n",
      "Task 4 is completed.\n",
      "Result from Task 4\n",
      "All tasks completed in 4.06 seconds.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_task(task_id):\n",
    "    print(f\"Task {task_id} is starting...\")\n",
    "    time.sleep(2)  # Simulate a time-consuming task\n",
    "    print(f\"Task {task_id} is completed.\")\n",
    "    return f\"Result from Task {task_id}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a thread pool with 3 threads\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # Submit multiple tasks to the thread pool\n",
    "        future_results = [executor.submit(process_task, i) for i in range(5)]\n",
    "        \n",
    "        # Retrieve results as they complete\n",
    "        for future in future_results:\n",
    "            print(future.result())  # Get the result from the thread\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"All tasks completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_text_extract():\n",
    "    # Prompt the user for the URL to scrape\n",
    "    url = input(\"Enter the URL to scrape: \")\n",
    "\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return  # Exit the function if the webpage wasn't fetched successfully\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract specific elements (e.g., headlines, paragraphs)\n",
    "    headlines = soup.find_all('h1')  # Example: Extract all <h1> tags\n",
    "    for headline in headlines:\n",
    "        print(headline.text.strip())\n",
    "    \n",
    "    # Extract paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for para in paragraphs:\n",
    "        print(para.text.strip())\n",
    "\n",
    "# Call the function\n",
    "web_text_extract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to extracted_content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_text_extract(url, output_file):\n",
    "    # Set headers to make the request look like it's coming from a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Fetch the webpage content with the custom headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text  # Now html_content is defined only if the request is successful\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return  # Exit the function if the webpage wasn't fetched successfully\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Open a text file to write the output\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        # Extract specific elements (e.g., headlines, paragraphs)\n",
    "        headlines = soup.find_all('h1')  # Example: Extract all <h1> tags\n",
    "        for headline in headlines:\n",
    "            file.write(headline.text.strip() + '\\n')\n",
    "\n",
    "        # Example: Extract paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for para in paragraphs:\n",
    "            file.write(para.text.strip() + '\\n')\n",
    "\n",
    "    print(f\"Text extracted and saved to {output_file}\")\n",
    "\n",
    "# Now you can call the function with a URL and output file name like:\n",
    "url = input(\"Enter the URL to scrape: \")\n",
    "output_file = \"extracted_content.txt\"\n",
    "web_text_extract(url, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to extracted_content.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_text_extract(url, output_file):\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Open a text file to write the output\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        # Extract specific elements (e.g., headlines, paragraphs)\n",
    "        headlines = soup.find_all('h1')  # Example: Extract all <h1> tags\n",
    "        for headline in headlines:\n",
    "            file.write(headline.text.strip() + '\\n')\n",
    "\n",
    "        # Example: Extract paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for para in paragraphs:\n",
    "            file.write(para.text.strip() + '\\n')\n",
    "\n",
    "    print(f\"Text extracted and saved to {output_file}\")\n",
    "\n",
    "# Now you can call the function with a URL and output file name like:\n",
    "url = input(\"Enter the URL to scrape: \")\n",
    "output_file = \"extracted_content.txt\"\n",
    "web_text_extract(url, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ewour Waste Mangement\n",
      "\t\t\t\t\t\n",
      "Contact Us: 0120-4250559  \n",
      "Toll Free: 18008890452\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "More About Us\n",
      "Ewour Waste Management And Its Professionals To Provide Tailored Recycling Solutions For Waste Electrical And Electronic Equipment Our Core Activities Combine State-Of-The-Art Technology With In-Depth Knowledge OF Environment And Waste Management Techniques To Provide Reuse Solutions And End Recovery Of Precious Metals From ELectronic Wastes Ewour thrive for eco friendly recycling and reuse of electronic wastes. It is about time that we become meticulous towards waste management and Earth conservation. Ewour joins millions of hands across the nation working in line with the E- waste rules 2016 along with The Swachh bharat initiative to envision and make India a cleaner and greener nation. Ewour have also implemented reverse logistics for recycling of E- waste\n",
      "Guaranteed that all of your universal waste management is performed safely and responsibly.\n",
      "We offer business pickup services to safely recycle your electronics in a safe manner.\n",
      "We work with non-profits, businesses, and other organizations to host community e-waste events.\n",
      "For old equipment\n",
      "What we do\n",
      "Clients can simply schedule their hard drive destruction online and through our website.\n",
      "Users quickly replace their electronic devices with newer, faster & stronger gadgets on the market.\n",
      "Buyers are welcome to leave their best offer on available electronic products.\n",
      "About us\n",
      "Our firm strives to eliminate the urban plague of e-waste pollution. We have designed and manifested this startup to efficiently support EEE manufacturers, producers as well as relevant businesses that need e-waste and plastic recycling and management solutions. Our robust processes includes dismantling of electronics waste in a highly secured plant, with well-equipped machinery and cutting-edge technology.\n",
      "Mr Ravi Choudhary is a man of great vision and this firm endeavors in supporting the Swach Bharat Movement. Although, e-waste management is one of the biggest evils in the realm of pollution, there aren't many corporations catering to solve this issue.\n",
      "Please do not put any of these items in your blue recycling cart. Recycle More curbside pickups are by appointment only.\n",
      "Accepted Items:\n",
      "India is today the 5th largest generator of e-waste in the world. Extended Producer Responsibility (EPR), coupled with targets for Producers, offers huge opportunity for product take-back from the market and from Producers. Besides, huge prospects in e-waste market are waiting to be tapped.A Collection centre offers the scope of a handsome income in addition to contributing to the cause of green environment. As part of latest E-waste Management Rules (2016), an e-waste collection centre does not require any license from the State Pollution Control Board. This presents a lucrative opportunity to the entrepreneurs who seek to reap good returns from the burgeoning e-waste market.\n",
      "Be a part of the revolution and an upcoming industry. Associate with Ewour, Indiaâs Producer Responsibility Organization (PRO) and be an active contributor to a pollution free earth.\n",
      "Frequently asked qaestions\n",
      "E-wasteâ means electrical and electronic equipment, whole or in part discarded as waste by the consumer or bulk consumer as well as rejects from manufacturing, refurbishment and repair processes.\n",
      "The total generation of e-waste in India in 2016 was to the tune of 18 lakh metric tonnes by some estimates. It is likely to reach 52 lakh metrics tonnes by 2020 growing at a compound annual growth rate (CAGR) of about 30%.\n",
      "According to UN studies, in 2016, 44.7 million metric tonnes of e-waste were generated, in which 40.7% of world e-waste was generated in Asia\n",
      "what clients say\n",
      "Director\n",
      "CEO\n",
      "Designer\n",
      "Blog Posts\n",
      "Clients can simply schedule their hard drive destruction online and through our website.\n",
      "Users quickly replace their electronic devices with newer, faster & stronger gadgets on the market.\n",
      "Buyers are welcome to leave their best offer on available electronic products.\n",
      "Family-owned company from India serving individuals and businesses in India.\n",
      "Name *\n",
      "Email *\n",
      "Message\n",
      "\n",
      "IIT Delhi Researchers Develop Technology To Manage, Recycle E-Waste\n",
      "The Global E-waste Monitor 2020 report found that the world dumped a record 53.6 million tonnes of e-waste last year. Just 17.4 percent was recycled.\n",
      "The global volume of electronic waste is expected to reach 52.2 million tonnes or 6.8 kg per person by 2021\n",
      "© Copyright 2017 All Rights Reserved\n"
     ]
    }
   ],
   "source": [
    "web_text_extract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "# url = 'https://python.langchain.com/v0.2/docs/concepts/'\n",
    "# url='https://www.linkedin.com/search/results/people/?keywords=firstname%20lastname&origin=CLUSTER_EXPANSION&sid=%40H3'\n",
    "url=input('https://ewour.in/')\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract specific elements (e.g., headlines, paragraphs)\n",
    "headlines = soup.find_all('h1')  # Example: Extract all <h1> tags\n",
    "for headline in headlines:\n",
    "    print(headline.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ewour Waste Mangement\n",
      "\t\t\t\t\t\n",
      "Contact Us: 0120-4250559  \n",
      "Toll Free: 18008890452\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "We can solve your corporate IT disposition needs quickly and professionally. Save Your community, Save Your planet\n",
      "More About Us\n",
      "Ewour Waste Management And Its Professionals To Provide Tailored Recycling Solutions For Waste Electrical And Electronic Equipment Our Core Activities Combine State-Of-The-Art Technology With In-Depth Knowledge OF Environment And Waste Management Techniques To Provide Reuse Solutions And End Recovery Of Precious Metals From ELectronic Wastes Ewour thrive for eco friendly recycling and reuse of electronic wastes. It is about time that we become meticulous towards waste management and Earth conservation. Ewour joins millions of hands across the nation working in line with the E- waste rules 2016 along with The Swachh bharat initiative to envision and make India a cleaner and greener nation. Ewour have also implemented reverse logistics for recycling of E- waste\n",
      "Guaranteed that all of your universal waste management is performed safely and responsibly.\n",
      "We offer business pickup services to safely recycle your electronics in a safe manner.\n",
      "We work with non-profits, businesses, and other organizations to host community e-waste events.\n",
      "For old equipment\n",
      "What we do\n",
      "Clients can simply schedule their hard drive destruction online and through our website.\n",
      "Users quickly replace their electronic devices with newer, faster & stronger gadgets on the market.\n",
      "Buyers are welcome to leave their best offer on available electronic products.\n",
      "About us\n",
      "Our firm strives to eliminate the urban plague of e-waste pollution. We have designed and manifested this startup to efficiently support EEE manufacturers, producers as well as relevant businesses that need e-waste and plastic recycling and management solutions. Our robust processes includes dismantling of electronics waste in a highly secured plant, with well-equipped machinery and cutting-edge technology.\n",
      "Mr Ravi Choudhary is a man of great vision and this firm endeavors in supporting the Swach Bharat Movement. Although, e-waste management is one of the biggest evils in the realm of pollution, there aren't many corporations catering to solve this issue.\n",
      "Please do not put any of these items in your blue recycling cart. Recycle More curbside pickups are by appointment only.\n",
      "Accepted Items:\n",
      "India is today the 5th largest generator of e-waste in the world. Extended Producer Responsibility (EPR), coupled with targets for Producers, offers huge opportunity for product take-back from the market and from Producers. Besides, huge prospects in e-waste market are waiting to be tapped.A Collection centre offers the scope of a handsome income in addition to contributing to the cause of green environment. As part of latest E-waste Management Rules (2016), an e-waste collection centre does not require any license from the State Pollution Control Board. This presents a lucrative opportunity to the entrepreneurs who seek to reap good returns from the burgeoning e-waste market.\n",
      "Be a part of the revolution and an upcoming industry. Associate with Ewour, Indiaâs Producer Responsibility Organization (PRO) and be an active contributor to a pollution free earth.\n",
      "Frequently asked qaestions\n",
      "E-wasteâ means electrical and electronic equipment, whole or in part discarded as waste by the consumer or bulk consumer as well as rejects from manufacturing, refurbishment and repair processes.\n",
      "The total generation of e-waste in India in 2016 was to the tune of 18 lakh metric tonnes by some estimates. It is likely to reach 52 lakh metrics tonnes by 2020 growing at a compound annual growth rate (CAGR) of about 30%.\n",
      "According to UN studies, in 2016, 44.7 million metric tonnes of e-waste were generated, in which 40.7% of world e-waste was generated in Asia\n",
      "what clients say\n",
      "Director\n",
      "CEO\n",
      "Designer\n",
      "Blog Posts\n",
      "Clients can simply schedule their hard drive destruction online and through our website.\n",
      "Users quickly replace their electronic devices with newer, faster & stronger gadgets on the market.\n",
      "Buyers are welcome to leave their best offer on available electronic products.\n",
      "Family-owned company from India serving individuals and businesses in India.\n",
      "Name *\n",
      "Email *\n",
      "Message\n",
      "\n",
      "IIT Delhi Researchers Develop Technology To Manage, Recycle E-Waste\n",
      "The Global E-waste Monitor 2020 report found that the world dumped a record 53.6 million tonnes of e-waste last year. Just 17.4 percent was recycled.\n",
      "The global volume of electronic waste is expected to reach 52.2 million tonnes or 6.8 kg per person by 2021\n",
      "© Copyright 2017 All Rights Reserved\n"
     ]
    }
   ],
   "source": [
    "# Example: Extract paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "for para in paragraphs:\n",
    "    print(para.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptual guide\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up the browser driver (e.g., ChromeDriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the webpage\n",
    "url = 'https://python.langchain.com/v0.2/docs/concepts/'\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Extract content after rendering\n",
    "content = driver.page_source\n",
    "\n",
    "# Parse using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Extract desired elements\n",
    "elements = soup.find_all('h1')\n",
    "for element in elements:\n",
    "    print(element.text.strip())\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping profile: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"h1.text-heading-xlarge\"}\n",
      "  (Session info: chrome=131.0.6778.86); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7EE666CB5+28821]\n",
      "\t(No symbol) [0x00007FF7EE5D3840]\n",
      "\t(No symbol) [0x00007FF7EE47578A]\n",
      "\t(No symbol) [0x00007FF7EE4C91BE]\n",
      "\t(No symbol) [0x00007FF7EE4C94AC]\n",
      "\t(No symbol) [0x00007FF7EE512647]\n",
      "\t(No symbol) [0x00007FF7EE4EF33F]\n",
      "\t(No symbol) [0x00007FF7EE50F412]\n",
      "\t(No symbol) [0x00007FF7EE4EF0A3]\n",
      "\t(No symbol) [0x00007FF7EE4BA778]\n",
      "\t(No symbol) [0x00007FF7EE4BB8E1]\n",
      "\tGetHandleVerifier [0x00007FF7EE99FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7EE9B741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7EE9AB5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7EE72BDBB+835995]\n",
      "\t(No symbol) [0x00007FF7EE5DEB5F]\n",
      "\t(No symbol) [0x00007FF7EE5DA814]\n",
      "\t(No symbol) [0x00007FF7EE5DA9AD]\n",
      "\t(No symbol) [0x00007FF7EE5CA199]\n",
      "\tBaseThreadInitThunk [0x00007FF96DC37374+20]\n",
      "\tRtlUserThreadStart [0x00007FF96DEFCC91+33]\n",
      "\n",
      "Error scraping profile: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"h1.text-heading-xlarge\"}\n",
      "  (Session info: chrome=131.0.6778.86); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7EE666CB5+28821]\n",
      "\t(No symbol) [0x00007FF7EE5D3840]\n",
      "\t(No symbol) [0x00007FF7EE47578A]\n",
      "\t(No symbol) [0x00007FF7EE4C91BE]\n",
      "\t(No symbol) [0x00007FF7EE4C94AC]\n",
      "\t(No symbol) [0x00007FF7EE512647]\n",
      "\t(No symbol) [0x00007FF7EE4EF33F]\n",
      "\t(No symbol) [0x00007FF7EE50F412]\n",
      "\t(No symbol) [0x00007FF7EE4EF0A3]\n",
      "\t(No symbol) [0x00007FF7EE4BA778]\n",
      "\t(No symbol) [0x00007FF7EE4BB8E1]\n",
      "\tGetHandleVerifier [0x00007FF7EE99FCAD+3408013]\n",
      "\tGetHandleVerifier [0x00007FF7EE9B741F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF7EE9AB5FD+3455453]\n",
      "\tGetHandleVerifier [0x00007FF7EE72BDBB+835995]\n",
      "\t(No symbol) [0x00007FF7EE5DEB5F]\n",
      "\t(No symbol) [0x00007FF7EE5DA814]\n",
      "\t(No symbol) [0x00007FF7EE5DA9AD]\n",
      "\t(No symbol) [0x00007FF7EE5CA199]\n",
      "\tBaseThreadInitThunk [0x00007FF96DC37374+20]\n",
      "\tRtlUserThreadStart [0x00007FF96DEFCC91+33]\n",
      "\n",
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Wait for login to complete\n",
    "\n",
    "def scrape_profile(driver, profile_url):\n",
    "    \"\"\"\n",
    "    Scrapes first name, last name, city, and company from a LinkedIn profile page.\n",
    "    \"\"\"\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(3)  # Wait for page to load\n",
    "\n",
    "    # Extract information (ensure these selectors match LinkedIn's current structure)\n",
    "    try:\n",
    "        name = driver.find_element(By.CSS_SELECTOR, \"h1.text-heading-xlarge\").text\n",
    "        location = driver.find_element(By.CSS_SELECTOR, \".text-body-small.inline\").text\n",
    "        company = driver.find_element(By.CSS_SELECTOR, \".pv-entity__secondary-title\").text\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping profile: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Split name into first and last\n",
    "    name_parts = name.split()\n",
    "    first_name = name_parts[0]\n",
    "    last_name = \" \".join(name_parts[1:])\n",
    "\n",
    "    # Return scraped data\n",
    "    return {\n",
    "        \"First Name\": first_name,\n",
    "        \"Last Name\": last_name,\n",
    "        \"City\": location,\n",
    "        \"Company\": company,\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up the browser driver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # LinkedIn login credentials\n",
    "        email = \"ashwani.digitalnotebook@gmail.com\"\n",
    "        password = \"Dayachand@7037\"\n",
    "\n",
    "        # Log in to LinkedIn\n",
    "        login_to_linkedin(driver, email, password)\n",
    "\n",
    "        # List of profile URLs to scrape\n",
    "        profile_urls = [\n",
    "            \"https://www.linkedin.com/in/example1\",\n",
    "            \"https://www.linkedin.com/in/example2\",\n",
    "        ]\n",
    "\n",
    "        # Scrape each profile\n",
    "        scraped_data = []\n",
    "        for profile_url in profile_urls:\n",
    "            data = scrape_profile(driver, profile_url)\n",
    "            if data:\n",
    "                scraped_data.append(data)\n",
    "                time.sleep(2)  # Respectful delay between requests\n",
    "\n",
    "        # Save data to a CSV file\n",
    "        if scraped_data:\n",
    "            df = pd.DataFrame(scraped_data)\n",
    "            df.to_csv(\"linkedin_profiles.csv\", index=False)\n",
    "            print(\"Data saved to linkedin_profiles.csv\")\n",
    "        else:\n",
    "            print(\"No data scraped.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Wait for login to complete\n",
    "\n",
    "email = \"ashwani.digitalnotebook@gmail.com\"\n",
    "password = \"Dayachand@7037\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data was scraped.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Wait for login to complete\n",
    "\n",
    "def scrape_linkedin_search_results(driver, search_url, num_scrolls=5, rate_limit=2):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn search results for profiles based on a search query.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance.\n",
    "    - search_url: URL of the LinkedIn search results page.\n",
    "    - num_scrolls: Number of scrolls to load more results (default 5).\n",
    "    - rate_limit: Time in seconds to wait between actions (default 2 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing scraped information.\n",
    "    \"\"\"\n",
    "    # Go to the search results page\n",
    "    driver.get(search_url)\n",
    "    time.sleep(rate_limit)\n",
    "\n",
    "    # Scroll to load more results\n",
    "    for _ in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(rate_limit)\n",
    "\n",
    "    # Extract profile information\n",
    "    profiles = driver.find_elements(By.CLASS_NAME, \"entity-result__content\")\n",
    "    scraped_data = []\n",
    "    \n",
    "    for profile in profiles:\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = profile.find_element(By.CLASS_NAME, \"entity-result__title-text\").text\n",
    "            first_name, last_name = name.split(\" \", 1)\n",
    "\n",
    "            # Extract headline (used to infer the company or role)\n",
    "            headline = profile.find_element(By.CLASS_NAME, \"entity-result__primary-subtitle\").text\n",
    "\n",
    "            # Extract location (if available)\n",
    "            location = profile.find_element(By.CLASS_NAME, \"entity-result__secondary-subtitle\").text\n",
    "\n",
    "            scraped_data.append({\n",
    "                \"First Name\": first_name,\n",
    "                \"Last Name\": last_name,\n",
    "                \"City\": location,\n",
    "                \"Company\": headline,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data from a profile: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(scraped_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # LinkedIn login credentials\n",
    "        email = \"ashwani.digitalnotebook@gmail.com\"\n",
    "        password = \"Dayachand@7037\"\n",
    "\n",
    "        # Log in to LinkedIn\n",
    "        login_to_linkedin(driver, email, password)\n",
    "\n",
    "        # LinkedIn search URL (replace with your actual search results URL)\n",
    "        search_url = \"https://www.linkedin.com/search/results/people/?keywords=firstname%20lastname\"\n",
    "\n",
    "        # Scrape search results\n",
    "        data = scrape_linkedin_search_results(driver, search_url, num_scrolls=3, rate_limit=3)\n",
    "\n",
    "        # Save to CSV\n",
    "        if not data.empty:\n",
    "            data.to_csv(\"linkedin_search_results.csv\", index=False)\n",
    "            print(\"Data saved to linkedin_search_results.csv\")\n",
    "        else:\n",
    "            print(\"No data was scraped.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for login to complete\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"global-nav-search\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for login to complete\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"global-nav-search\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "        # LinkedIn login credentials\n",
    "email = \"ashwani.digitalnotebook@gmail.com\"\n",
    "password = \"Dayachand@7037\"\n",
    "login_to_linkedin(driver,email,password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeoutException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 57\u001b[0m, in \u001b[0;36mscrape_linkedin_search_results\u001b[1;34m(driver, search_url, num_scrolls, rate_limit)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity-result__content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit loop if successful\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m search_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.linkedin.com/search/results/people/?keywords=firstname\u001b[39m\u001b[38;5;132;01m%20la\u001b[39;00m\u001b[38;5;124mstname\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Scrape search results\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_linkedin_search_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_scrolls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[1;32mIn[16], line 61\u001b[0m, in \u001b[0;36mscrape_linkedin_search_results\u001b[1;34m(driver, search_url, num_scrolls, rate_limit)\u001b[0m\n\u001b[0;32m     57\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     58\u001b[0m         EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity-result__content\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     59\u001b[0m     )\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit loop if successful\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mTimeoutException\u001b[49m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# Last attempt failed\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfiles could not be loaded after 3 attempts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TimeoutException' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs into LinkedIn using provided credentials.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter email\n",
    "    email_input = driver.find_element(By.ID, \"username\")\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Enter password\n",
    "    password_input = driver.find_element(By.ID, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Click login button\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for login to complete\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"global-nav-search\"))\n",
    "    )\n",
    "\n",
    "def scrape_linkedin_search_results(driver, search_url, num_scrolls=5, rate_limit=2):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn search results for profiles based on a search query.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance.\n",
    "    - search_url: URL of the LinkedIn search results page.\n",
    "    - num_scrolls: Number of scrolls to load more results (default 5).\n",
    "    - rate_limit: Time in seconds to wait between actions (default 2 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing scraped information.\n",
    "    \"\"\"\n",
    "    # Go to the search results page\n",
    "    driver.get(search_url)\n",
    "    time.sleep(rate_limit)\n",
    "\n",
    "    # Scroll to load more results\n",
    "    for _ in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(rate_limit)\n",
    "\n",
    "    # Retry logic for loading profiles\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, \"entity-result__content\"))\n",
    "            )\n",
    "            break  # Exit loop if successful\n",
    "        except TimeoutException:\n",
    "            if attempt == 2:  # Last attempt failed\n",
    "                print(\"Profiles could not be loaded after 3 attempts.\")\n",
    "                raise\n",
    "            print(\"Retrying to load profiles...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "    # Extract profile information\n",
    "    profiles = driver.find_elements(By.CLASS_NAME, \"entity-result__content\")\n",
    "    print(f\"Found {len(profiles)} profiles on the page.\")\n",
    "\n",
    "    if not profiles:\n",
    "        print(\"No profiles found. Check the class name or login status.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no profiles found\n",
    "\n",
    "    scraped_data = []\n",
    "    for profile in profiles:\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = profile.find_element(By.CLASS_NAME, \"entity-result__title-text\").text\n",
    "            first_name, last_name = name.split(\" \", 1)\n",
    "\n",
    "            # Extract headline (used to infer the company or role)\n",
    "            headline = profile.find_element(By.CLASS_NAME, \"entity-result__primary-subtitle\").text\n",
    "\n",
    "            # Extract location (if available)\n",
    "            location = profile.find_element(By.CLASS_NAME, \"entity-result__secondary-subtitle\").text\n",
    "\n",
    "            scraped_data.append({\n",
    "                \"First Name\": first_name,\n",
    "                \"Last Name\": last_name,\n",
    "                \"City\": location,\n",
    "                \"Company\": headline,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data from a profile: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(scraped_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # LinkedIn login credentials\n",
    "        email = \"ashwani.digitalnotebook@gmail.com\"\n",
    "        password = \"Dayachand@7037\"\n",
    "\n",
    "        # Log in to LinkedIn\n",
    "        login_to_linkedin(driver, email, password)\n",
    "\n",
    "        # LinkedIn search URL (replace with your actual search results URL)\n",
    "        search_url = \"https://www.linkedin.com/search/results/people/?keywords=firstname%20lastname\"\n",
    "\n",
    "        # Scrape search results\n",
    "        data = scrape_linkedin_search_results(driver, search_url, num_scrolls=3, rate_limit=3)\n",
    "\n",
    "        # Save to CSV\n",
    "        if not data.empty:\n",
    "            data.to_csv(\"linkedin_search_results.csv\", index=False)\n",
    "            print(\"Data saved to linkedin_search_results.csv\")\n",
    "        else:\n",
    "            print(\"No data was scraped. Check if locators need to be updated or if LinkedIn blocked the request.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_search_results(driver, search_url, num_scrolls=5, rate_limit=2):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn search results for profiles based on a search query.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance.\n",
    "    - search_url: URL of the LinkedIn search results page.\n",
    "    - num_scrolls: Number of scrolls to load more results (default 5).\n",
    "    - rate_limit: Time in seconds to wait between actions (default 2 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing scraped information.\n",
    "    \"\"\"\n",
    "    # Go to the search results page\n",
    "    driver.get(search_url)\n",
    "    time.sleep(rate_limit)\n",
    "\n",
    "    # Scroll to load more results\n",
    "    for _ in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(rate_limit)\n",
    "\n",
    "    # Retry logic for loading profiles\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, \"entity-result__content\"))\n",
    "            )\n",
    "            break  # Exit loop if successful\n",
    "        except TimeoutException:\n",
    "            if attempt == 2:  # Last attempt failed\n",
    "                print(\"Profiles could not be loaded after 3 attempts.\")\n",
    "                raise\n",
    "            print(\"Retrying to load profiles...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "    # Extract profile information\n",
    "    profiles = driver.find_elements(By.CLASS_NAME, \"entity-result__content\")\n",
    "    print(f\"Found {len(profiles)} profiles on the page.\")\n",
    "\n",
    "    if not profiles:\n",
    "        print(\"No profiles found. Check the class name or login status.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no profiles found\n",
    "\n",
    "    scraped_data = []\n",
    "    for profile in profiles:\n",
    "        try:\n",
    "            # Extract name\n",
    "            name = profile.find_element(By.CLASS_NAME, \"entity-result__title-text\").text\n",
    "            first_name, last_name = name.split(\" \", 1)\n",
    "\n",
    "            # Extract headline (used to infer the company or role)\n",
    "            headline = profile.find_element(By.CLASS_NAME, \"entity-result__primary-subtitle\").text\n",
    "\n",
    "            # Extract location (if available)\n",
    "            location = profile.find_element(By.CLASS_NAME, \"entity-result__secondary-subtitle\").text\n",
    "\n",
    "            scraped_data.append({\n",
    "                \"First Name\": first_name,\n",
    "                \"Last Name\": last_name,\n",
    "                \"City\": location,\n",
    "                \"Company\": headline,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data from a profile: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
